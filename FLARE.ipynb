{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41341966-685e-48e7-aff0-bf378566fa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/idsia/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "access_token = \"secret-token\"\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6bd451-73c8-4294-8a10-b8c304194378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel, TFBertForQuestionAnswering,TFAutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739e56b7-8939-46e3-8396-fb90e65fd809",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_use = 0\n",
    "st = \"cuda:\"+str(GPU_use)\n",
    "torch.cuda.set_device(GPU_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d58058-1799-4e3a-b21a-f0b0e6b4822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('marcomaccarini/blind_3_7_tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de81cd7-01f5-4ce1-b7f0-13caade7a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ea47cc-323c-4c9f-a321-e06ac788a8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c96c464bb604c8aa4c7e9d476a63403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = 'meta-llama/Meta-Llama-3-8B'\n",
    "tokr = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"marcomaccarini/FLARE\", torch_dtype=torch.bfloat16, device_map=GPU_use,token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8b26b7-9b4e-4dfd-9824-85aee170ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "USER: {}\n",
    "===\n",
    "{}\n",
    "ASSISTANT:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86ae860-deba-4ab1-b83e-fa63ed0117d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_prompt(d): \n",
    "    return fmt.format(d[\"context\"], d[\"question\"])\n",
    "def question(table, quest):\n",
    "    tst = dict(**trn[8])\n",
    "    tst['context'] = table\n",
    "    tst['question'] = quest\n",
    "    return sql_prompt(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa218ef1-e92a-41b8-ad48-633a87d1d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'table([ eof x: 345 y: 279 z: 348, gripper: open , black-cup x: -154 y: 127 z: 180, white-cup x: -115 y: 359 z: 60, box x: -310 y: 134 z: 100, green-cylinder x: 125 y: -32 z: 80 or: 142, green-cube x: -190 y: -190 z: 80 or: 83, grey-cube x: 536 y: -222 z: 80 or: 96, red-cube x: -32 y: 58 z: 80 or: 157, yellow-ball x: -21 y: 30 z: 20 or: 41, banana x: 2 y: 53 z: 20 or: 9, remote x: -48 y: 31 z: 30 or: 69, pen x: -53 y: -59 z: 10 or: 174 ])'\n",
    "q = 'pick green-cube and place to black-cup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de063923-ffb0-478b-99df-014438d1c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.026280164718628\n",
      "CPU times: user 1.89 s, sys: 142 ms, total: 2.03 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "start = time.time()\n",
    "test = question(t,q)\n",
    "toks = tokr(test, return_tensors=\"pt\")\n",
    "res = model.generate(**toks.to(st), max_new_tokens=100, top_p = 0).to('cpu')\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943b5ec2-296e-4256-a883-7484744ed0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "USER: table([ eof x: 345 y: 279 z: 348, gripper: open, black-cup x: -154 y: 127 z: 180, white-cup x: -115 y: 359 z: 60, box x: -310 y: 134 z: 100, green-cylinder x: 125 y: -32 z: 80 or: 142, green-cube x: -190 y: -190 z: 80 or: 83, grey-cube x: 536 y: -222 z: 80 or: 96, red-cube x: -32 y: 58 z: 80 or: 157, yellow-ball x: -21 y: 30 z: 20 or: 41, banana x: 2 y: 53 z: 20 or: 9, remote x: -48 y: 31 z: 30 or: 69, pen x: -53 y: -59 z: 10 or: 174 ])\n",
      "===\n",
      "pick green-cube and place to black-cup\n",
      "ASSISTANT: -190+0;-190+0;80+300;83,-190+0;-190+0;80+0;83,close,-190+0;-190+0;80+300;83,-154+0;127+0;180+300;0,-154+0;127+0;180+80;0,open,home<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(tokr.batch_decode(res)[0].replace(\"*\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d325f45-d933-4410-aed5-0cd449258563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4815dfd-0596-42ef-a800-417003d8fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = tokr.batch_decode(res)[0].split(\":\")[-1].replace(\" \",\"\").replace(\")<|end_of_text|>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9b2171-8edc-4bd1-bb9c-f5c63b6a3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inference(xyz_theta):\n",
    "    if '+' in xyz_theta: \n",
    "        splitted = xyz_theta.split(\"+\")\n",
    "        x = int(splitted[0])+int(splitted[1])\n",
    "    elif '-' in xyz_theta:\n",
    "        splitted = xyz_theta.split(\"-\")\n",
    "        x = int(splitted[0])-int(splitted[1])\n",
    "    else:\n",
    "        splitted = xyz_theta.split(\"-\")\n",
    "        x = int(splitted[0])\n",
    "    return x\n",
    "\n",
    "def clean_inference(xyz_theta):\n",
    "    a = []\n",
    "    for elements in xyz_theta:\n",
    "        if elements != \"\":\n",
    "            a.append(elements)\n",
    "    return a\n",
    "\n",
    "step = []\n",
    "for c in coord.split(\",\"): \n",
    "    xyz_theta = clean_inference(c.split(\";\"))\n",
    "    #print(xyz_theta)\n",
    "    if len(xyz_theta)==1:\n",
    "        step.append(xyz_theta[0].replace(\"<|end_of_text|>\",\"\"))\n",
    "    elif len(xyz_theta)==4: \n",
    "        x = parse_inference(xyz_theta[0])\n",
    "        y = parse_inference(xyz_theta[1])\n",
    "        z = parse_inference(xyz_theta[2])\n",
    "        step.append([x,y,z, int(xyz_theta[3])])\n",
    "    else:\n",
    "        step.append([int(xyz_theta[0]),int(xyz_theta[1]), int(xyz_theta[2].replace(\"<|end_of_text|>\",\"\")), 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77fbfcc-4d1d-420f-a5c2-1c898460645d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-190, -190, 380, 83],\n",
       " [-190, -190, 80, 83],\n",
       " 'close',\n",
       " [-190, -190, 380, 83],\n",
       " [-154, 127, 480, 0],\n",
       " [-154, 127, 260, 0],\n",
       " 'open',\n",
       " 'home']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c39152-be14-4f3d-9132-f950ecacb2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80628f69-b775-4e3c-af6a-525e2e1efcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
